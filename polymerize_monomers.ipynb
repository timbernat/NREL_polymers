{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logging and Shell\n",
    "import logging\n",
    "\n",
    "from polymerist.mdtools.openfftools.partialcharge import molchargers\n",
    "logging.basicConfig(\n",
    "    level=logging.ERROR,\n",
    "    force=True\n",
    ")\n",
    "\n",
    "## Generic imports\n",
    "from typing import Any, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "## Numeric imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## File I/O\n",
    "import json, pickle\n",
    "from pathlib import Path\n",
    "from ast import literal_eval\n",
    "\n",
    "# Cheminformatics\n",
    "from rdkit import Chem\n",
    "\n",
    "from openmm.unit import nanometer, angstrom, Quantity\n",
    "\n",
    "from openff.toolkit import Molecule, Topology, ForceField\n",
    "from openff.toolkit.utils.exceptions import (\n",
    "    UnassignedChemistryInPDBError,\n",
    "    IncorrectNumConformersWarning,\n",
    ")\n",
    "\n",
    "# Custom Imports\n",
    "from polymerist.genutils.containers import RecursiveDict\n",
    "from polymerist.genutils.fileutils import filetree\n",
    "from polymerist.genutils.fileutils.pathutils import assemble_path\n",
    "from polymerist.duration import Duration, Timer\n",
    "from polymerist.unitutils.interop import openmm_to_openff, openff_to_openmm\n",
    "\n",
    "from polymerist.maths.greek import GREEK_PREFIXES\n",
    "from polymerist.rdutils.rdprops import copy_rd_props\n",
    "from polymerist.rdutils.rdcoords.tiling import rdmol_effective_radius\n",
    "from polymerist.rdutils.reactions import reactions, reactors\n",
    "\n",
    "from polymerist.mdtools.openfftools.partition import partition\n",
    "from polymerist.mdtools.openfftools import topology, boxvectors, FFDIR\n",
    "from polymerist.monomers import specification, MonomerGroup\n",
    "from polymerist.polymers import building\n",
    "\n",
    "# catch annoying warnings\n",
    "import warnings \n",
    "warnings.catch_warnings(record=True)\n",
    "warnings.filterwarnings('ignore', category=IncorrectNumConformersWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load monomer and rxn data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static Paths\n",
    "RAW_DATA_DIR  = Path('monomer_data_raw')\n",
    "FMT_DATA_DIR  = Path('monomer_data_formatted')\n",
    "PROC_DATA_DIR = Path('monomer_data_processed')\n",
    "RXN_FILES_DIR = Path('poly_rxns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data_path = PROC_DATA_DIR / '20231114_polyid_data_density_DP2-6 - 1,2 monomers_FILTERED.csv'\n",
    "# input_data_path = PROC_DATA_DIR / '20231114_polyid_data_density_DP2-6_-_1,2_monomers_FILTERED_NEWSTYLE.csv'\n",
    "# input_data_path = PROC_DATA_DIR / 'nipu_urethanes_FILTERED.csv'\n",
    "# input_data_path = PROC_DATA_DIR / 'monomer_data_MASTER.csv'\n",
    "input_data_path = PROC_DATA_DIR / 'monomer_data_MASTER_NEWSTYLE.csv'\n",
    "\n",
    "df = pd.read_csv(input_data_path, index_col=[0,1])\n",
    "df = df.replace(np.nan, None) # swap NaN values for explicit NoneTypes to simplify value write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-defined reactions with functional group and name backmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_first_n : Optional[int] = None # debug option to only take a handful of compounds from each family\n",
    "# take_first_n : Optional[int] = 5\n",
    "blacklisted_rxns = ['polyimide', 'polyvinyl_head_tail']\n",
    "\n",
    "banned_by_reaction = df.index.get_level_values(0).isin(blacklisted_rxns)\n",
    "df = df[~banned_by_reaction] # only keep fields which haven't been banned\n",
    "if take_first_n is not None:\n",
    "    df = df.head(take_first_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-generating monomer fragments and Topologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters for build process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MASTER_OUT_DIR = Path('polymer_improved')\n",
    "MASTER_OUT_DIR = Path('polymers_streamlined')\n",
    "\n",
    "DOPs : tuple[int] = (3,)\n",
    "charge_method : str = 'Espaloma-AM1-BCC'\n",
    "force_field_name : str = 'openff_unconstrained-2.0.0.offxml' # 'openff-2.0.0.offxml'\n",
    "max_num_atoms_array : tuple[int] = (10_000, 20_000,)\n",
    "\n",
    "switching_function : bool = False\n",
    "exclusion : Quantity = 0.0 * nanometer \n",
    "nonbond_cutoff : Quantity = 0.9 * nanometer\n",
    "\n",
    "clear_existing           : bool = True#False\n",
    "refragment               : bool = False  \n",
    "repolymerize_pdbs        : bool = False\n",
    "reparameterize           : bool = False\n",
    "reassign_partial_charges : bool = False\n",
    "perform_energy_min       : bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed parameters\n",
    "charger = molchargers.MolCharger.subclass_registry[charge_method]()\n",
    "forcefield = ForceField(FFDIR / force_field_name)\n",
    "\n",
    "min_box_dim : Quantity = 2 * nonbond_cutoff # should be at least twice the nonbonded cutoff to avoid self-interaction\n",
    "min_bbox = openmm_to_openff(min_box_dim * np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute build loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rich.live import Live\n",
    "from polybuild_utils import initialize_polymer_progress\n",
    "from polybuild_utils import HILL_REGEX, generate_smarts_fragments, generate_uniform_subpopulated_lattice\n",
    "\n",
    "# create directories\n",
    "MASTER_OUT_DIR.mkdir(exist_ok=True)\n",
    "if clear_existing:\n",
    "    filetree.clear_dir(MASTER_OUT_DIR)\n",
    "\n",
    "# set up data structures for global output\n",
    "failure_record = RecursiveDict()\n",
    "m2p_mismatches = RecursiveDict()\n",
    "\n",
    "# execute build loop\n",
    "num_successful : int = 0\n",
    "md_build_records : list[dict[str, Any]] = []\n",
    "\n",
    "group, (status_id, curr_compound_id, comp_progress_id) = initialize_polymer_progress(num_compounds=len(df))\n",
    "status_readout, compound_readout, compound_progress = group.renderables\n",
    "\n",
    "with Live(group, refresh_per_second=10) as live:\n",
    "    # ensure bars start at 0\n",
    "    for pbar in group.renderables: \n",
    "        for task_id in pbar.task_ids:\n",
    "            pbar.reset(task_id)\n",
    "\n",
    "    # iterate over all distinct chemistries by reaction mechanism\n",
    "    for (mechanism, polymer_name), row in df.iterrows():\n",
    "        # look up reactive groups and pathway by mechanism\n",
    "        rxn_pathway = reactions.AnnotatedReaction.from_smarts(row.reaction_smarts)\n",
    "        # temp_rxn_file = '_temp.rxn' # for some reason, the load from reaction gives the desired outcomes\n",
    "        # rxn_pathway.to_rxnfile(temp_rxn_file)\n",
    "        # rxn_pathway = reactions.AnnotatedReaction.from_rxnfile(temp_rxn_file)\n",
    "        reactor = reactors.PolymerizationReactor(rxn_pathway)\n",
    "        \n",
    "        # initialize output directories\n",
    "        compound_readout.update(curr_compound_id, polymer_name=polymer_name, mechanism=mechanism)\n",
    "        chem_dir : Path = MASTER_OUT_DIR  / polymer_name\n",
    "        chem_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # 0) load reactants with IUPAC names from chemical table\n",
    "        status_readout.update(status_id, action='Gathering reactants')\n",
    "        reactant_combomol = Chem.MolFromSmiles(row.reactant_smiles, sanitize=False)\n",
    "        reactants = Chem.GetMolFrags(reactant_combomol, asMols=True)\n",
    "\n",
    "        named_reactants = {}\n",
    "        for reactname, reactant in zip(literal_eval(row.iupac_names), reactants, strict=True):\n",
    "            Chem.SanitizeMol(reactant, sanitizeOps=specification.SANITIZE_AS_KEKULE)\n",
    "            named_reactants[reactname] = reactant\n",
    "\n",
    "        try:\n",
    "            # 1) use rxn template to polymerize monomers into all possible fragments\n",
    "            frag_path = assemble_path(chem_dir, polymer_name, extension='json')\n",
    "            if frag_path.exists() and not refragment: # if fragments have already been \n",
    "                status_readout.update(status_id, action='Loading pre-existing monomer fragments')\n",
    "                monogrp = MonomerGroup.from_file(frag_path)\n",
    "            else:\n",
    "                status_readout.update(status_id, action='Generating monomer fragments via reaction mechanism')\n",
    "                monogrp = generate_smarts_fragments(named_reactants, reactor=reactor)\n",
    "\n",
    "                status_readout.update(status_id, action='Saving monomer fragments...')\n",
    "                monogrp.to_file(frag_path)\n",
    "\n",
    "            for dop in DOPs:\n",
    "                nmer_name = f'{GREEK_PREFIXES[dop]}mer'\n",
    "                dop_dir : Path = chem_dir / nmer_name\n",
    "                dop_dir.mkdir(exist_ok=True)\n",
    "\n",
    "                # 2) Generate PDB file for linear chain from fragments\n",
    "                pdb_path : Path = assemble_path(dop_dir, polymer_name, extension='pdb')\n",
    "                if not pdb_path.exists() or repolymerize_pdbs:\n",
    "                    status_readout.update(status_id, action=f'Generating PDB file (with{\"\" if perform_energy_min else \"out\"} UFF energy minimization)')\n",
    "                    polymer = building.build_linear_polymer(monomers=monogrp, DOP=dop+1, sequence='BA', energy_minimize=perform_energy_min)  # \"BA\" is needed to make term groups align properly, DOP does not account for term group pair (hence the \"+1\")\n",
    "                    building.mbmol_to_openmm_pdb(pdb_path, polymer)\n",
    "                    \n",
    "                    # checking that my method produces the same results as M2P\n",
    "                    m2p_smiles = row.smiles_polymer_DP6\n",
    "                    if m2p_smiles is not None:\n",
    "                        m2p_mol = Chem.MolFromSmiles(m2p_smiles)\n",
    "                        workflow_smiles = polymer.to_smiles()\n",
    "                        workflow_mol    = Chem.MolFromSmiles(workflow_smiles)\n",
    "\n",
    "                        if not (workflow_mol.HasSubstructMatch(m2p_mol) or m2p_mol.HasSubstructMatch(workflow_mol)):\n",
    "                            m2p_mismatches[mechanism][polymer_name]['M2P_vers'] = m2p_smiles\n",
    "                            m2p_mismatches[mechanism][polymer_name]['workflow_vers'] = workflow_smiles\n",
    "\n",
    "                # 3a) Assign chemical info to PDB system\n",
    "                param_top_path : Path = assemble_path(dop_dir, polymer_name, extension='sdf')\n",
    "                if param_top_path.exists() and not reparameterize:\n",
    "                    status_readout.update(status_id, action='Loading parameterized single-mol Topology')\n",
    "                    offtop = topology.topology_from_sdf(param_top_path)\n",
    "                else:\n",
    "                    try:\n",
    "                        status_readout.update(status_id, action='Partitioning topology by fragments')\n",
    "                        offtop = Topology.from_pdb(pdb_path, _custom_substructures=monogrp.monomers)\n",
    "                        assert(partition(offtop)) # verify that a partition was possible\n",
    "                        topology.topology_to_sdf(param_top_path, offtop)\n",
    "                    except UnassignedChemistryInPDBError:\n",
    "                        failure_record['No substruct cover'][mechanism][polymer_name][dop] = monogrp\n",
    "                        continue # skip to next compounds, don't proceed with parameterization   \n",
    "                    except AssertionError:\n",
    "                        failure_record['No substruct partition'][mechanism][polymer_name][dop] = monogrp\n",
    "                        continue # skip to next compounds, don't proceed with parameterization   \n",
    "\n",
    "                offmol = topology.get_largest_offmol(offtop)\n",
    "                offmol.name = polymer_name\n",
    "\n",
    "                # 3b) Assign partial charges, if not already present\n",
    "                if not molchargers.has_partial_charges(offmol):\n",
    "                    status_readout.update(status_id, action=f'Assigning partial charges via {charger.CHARGING_METHOD}')\n",
    "                    cmol = charger.charge_molecule(offmol)\n",
    "                    unique_elems = re.findall(HILL_REGEX, cmol.hill_formula) # unique element names in same order as found in Hill formula\n",
    "                \n",
    "                # generate tiled lattices as specified\n",
    "                for max_num_atoms in max_num_atoms_array:\n",
    "                    lattice_str = f'sub_{max_num_atoms}_atoms'\n",
    "                    latt_dir : Path = dop_dir / lattice_str\n",
    "                    latt_dir.mkdir(exist_ok=True)\n",
    "\n",
    "                    int_lattice = generate_uniform_subpopulated_lattice(max_num_atoms, num_atoms_in_mol=cmol.n_atoms)\n",
    "                    r_eff = rdmol_effective_radius(cmol.to_rdkit())\n",
    "                    lattice = int_lattice.linear_transformation(2.0*r_eff*np.eye(3), as_coords=True) # scale integer lattice my effective diameter\n",
    "\n",
    "                    # create tiled version of parameterized topology\n",
    "                    with Timer() as topo_timer:\n",
    "                        status_readout.update(status_id, action=f'Generating tiled {lattice_str} topology')\n",
    "                        tiled_offtop = topology.topology_from_molecule_onto_lattice(cmol, lattice_points=lattice.points, rotate_randomly=True, unique_mol_ids=True)\n",
    "                        latt_top_path = assemble_path(latt_dir, lattice_str, postfix=polymer_name, extension='sdf')\n",
    "                        topology.topology_to_sdf(latt_top_path, tiled_offtop)\n",
    "\n",
    "                        latt_pdb_path = assemble_path(latt_dir, lattice_str, postfix=polymer_name, extension='pdb')\n",
    "                        tiled_offtop.to_file(latt_pdb_path)\n",
    "\n",
    "                    # generate appropriately-sized periodic box size, starting with the tight bounding box for the topology\n",
    "                    top_box_vectors = boxvectors.get_topology_bbox(tiled_offtop) # determine tight box size\n",
    "                    top_box_vectors = boxvectors.pad_box_vectors_uniform(top_box_vectors, exclusion) # apply periodic box (with padding) to Interchange\n",
    "                    top_box_vectors = np.maximum(min_bbox, top_box_vectors) # enusre the box is no smaller than the minimum determined by the cutoff distance\n",
    "\n",
    "                    top_box_vectors_omm = openff_to_openmm(top_box_vectors)\n",
    "                    box_vector_sizes = np.linalg.norm(top_box_vectors_omm, axis=1) * top_box_vectors_omm.unit # rows are each a distinct box vector\n",
    "                    box_vector_dict = {\n",
    "                        f'box_dim_{axis} ({size_quant.unit!s})' : size_quant._value\n",
    "                            for (axis, size_quant) in zip('xyz', box_vector_sizes)\n",
    "                    }\n",
    "\n",
    "                    # create and save Interchange for MD export\n",
    "                    with Timer() as inc_timer:\n",
    "                        status_readout.update(status_id, action=f'Creating {lattice_str} OpenFF Interchange')\n",
    "                        interchange = forcefield.create_interchange(tiled_offtop, charge_from_molecules=[cmol])\n",
    "                        interchange.box = top_box_vectors # apply periodic box to Interchange\n",
    "\n",
    "                        # configure nonbonded in Interchange to have correct cutoff and switching function width\n",
    "                        interchange['vdW'].switch_width = (1.0 if switching_function else 0.0) * angstrom\n",
    "                        interchange['vdW'           ].cutoff = nonbond_cutoff\n",
    "                        interchange['Electrostatics'].cutoff = nonbond_cutoff\n",
    "\n",
    "                    latt_inc_path = assemble_path(latt_dir, lattice_str, postfix=polymer_name, extension='pkl')\n",
    "                    with latt_inc_path.open('wb') as pklfile: # NOTE: pickled files must be read/written in binary mode\n",
    "                        pickle.dump(interchange, pklfile)\n",
    "\n",
    "                    # record information about MD build run to simplfiy resuming, analyzing, and benchmarking structure outputs\n",
    "                    md_build_entry = {\n",
    "                        'mechanism'                : mechanism,\n",
    "                        'polymer_name'             : polymer_name,\n",
    "                        'exper_density'            : row['Density'],\n",
    "                        'n_atoms_cap'              : lattice_str,\n",
    "                        'lattice_size'             : int_lattice.counts_along_dims_as_str(),\n",
    "                        'num_oligomers'            : lattice.n_points,\n",
    "                        'effective radius'         : r_eff,\n",
    "                        'oligomer_type'            : nmer_name,\n",
    "                        'n_atoms_in_topology'      : tiled_offtop.n_atoms,\n",
    "                        'unique_elems_in_topology' : unique_elems, \n",
    "                        'directory'                : str(latt_dir),\n",
    "                        'topology_path'            : str(latt_top_path),\n",
    "                        'topology_time'            : topo_timer.time_taken,\n",
    "                        'interchange_path'         : str(latt_inc_path),\n",
    "                        'interchange_time'         : inc_timer.time_taken,\n",
    "                    }\n",
    "                    md_build_entry.update(box_vector_dict)\n",
    "                    md_build_records.append(md_build_entry)\n",
    "\n",
    "                    md_build_entry_path = assemble_path(latt_dir, f'{lattice_str}_{nmer_name}_{polymer_name}', postfix='RECORD', extension='json')\n",
    "                    with md_build_entry_path.open('w') as record_file: # also save to disc individually, to allow reconstruction if loop fails haflway through\n",
    "                        json.dump(md_build_entry, record_file, indent=4)\n",
    "                \n",
    "            num_successful += 1\n",
    "\n",
    "        except Exception as other_error:\n",
    "            failure_record[other_error.__class__.__name__][mechanism][polymer_name] = str(other_error)\n",
    "        finally:\n",
    "            compound_progress.advance(comp_progress_id)\n",
    "            sleep(0.1) # needed to give final bar enough time to catch up\n",
    "    compound_readout.update(curr_compound_id, polymer_name=f'Completed! ({num_successful}/{len(df)} successful)')\n",
    "\n",
    "all_records_path = assemble_path(MASTER_OUT_DIR, 'build_records', extension='csv')\n",
    "md_build_records_table = pd.DataFrame.from_records(md_build_records)\n",
    "md_build_records_table.set_index(['mechanism', 'polymer_name'], inplace=True)\n",
    "md_build_records_table.to_csv(all_records_path)\n",
    "\n",
    "m2p_mismatch_path = assemble_path(MASTER_OUT_DIR, 'm2p_mismatches', extension='json')\n",
    "with m2p_mismatch_path.open('w') as m2p_mismatch_file:\n",
    "    json.dump(m2p_mismatches, m2p_mismatch_file, indent=4)\n",
    "print(m2p_mismatches)\n",
    "\n",
    "failures_path = assemble_path(MASTER_OUT_DIR, 'encountered_errors', extension='json')\n",
    "with failures_path.open('w') as failure_file:\n",
    "    json.dump(failure_record, failure_file, indent=4)\n",
    "print(failure_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openff-dev-updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
