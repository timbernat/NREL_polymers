{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating OpenMM and LAMMPS systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harvest and tabulate paths + info for all Interchange and Topology files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# MOL_MASTER_DIR = Path('polymer_structures')\n",
    "# MOL_MASTER_DIR = Path('polymer_validation')\n",
    "# MOL_MASTER_DIR = Path('polymer_revision')\n",
    "# MOL_MASTER_DIR = Path('polymer_update')\n",
    "# MOL_MASTER_DIR = Path('polymer_benchmark')\n",
    "# MOL_MASTER_DIR = Path('polymer_improved')\n",
    "# MOL_MASTER_DIR = Path('polymers_atom_limited')\n",
    "MOL_MASTER_DIR = Path('polymers_streamlined')\n",
    "build_records_path = MOL_MASTER_DIR / 'build_records.csv'\n",
    "\n",
    "# mol_file_frame = pd.read_csv(build_records_path, index_col=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for record_path in MOL_MASTER_DIR.glob('**/*_RECORD.json'):\n",
    "    if record_path.exists:\n",
    "        with record_path.open('r') as record_file:\n",
    "            mol_info = json.load(record_file)\n",
    "    mol_info['record_path'] = record_path\n",
    "\n",
    "    records.append(mol_info)\n",
    "\n",
    "mol_file_frame = pd.DataFrame.from_records(records)\n",
    "mol_file_frame.set_index(['mechanism', 'polymer_name'], inplace=True)\n",
    "for str_path_col in ('topology_path', 'interchange_path', 'directory'):\n",
    "    mol_file_frame[str_path_col] = mol_file_frame[str_path_col].map(Path) # de-stringify file Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "take_first_n : Optional[int] = None # debug option to only take a handful of compounds from each family\n",
    "if take_first_n is not None:\n",
    "    mol_file_frame = mol_file_frame.head(take_first_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create MD files from Interchange, evaluate starting energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute MD loop proper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import cProfile\n",
    "from rich.live import Live\n",
    "\n",
    "from time import sleep\n",
    "from gc import collect\n",
    "\n",
    "from openmm import XmlSerializer, Context\n",
    "from openmm.unit import kilojoule_per_mole\n",
    "\n",
    "from polymerist.genutils.containers import RecursiveDict\n",
    "from polymerist.genutils.textual.interpolation import insert_into_text_periodic\n",
    "from polymerist.genutils.fileutils.pathutils import assemble_path\n",
    "from polymerist.genutils.fileutils.jsonio.update import append_to_json\n",
    "\n",
    "from polymerist.duration import Timer\n",
    "from polymerist.lammpstools import lammpseval\n",
    "from polymerist.mdtools.openfftools import topology\n",
    "\n",
    "from polymerist.mdtools.openmmtools.thermo import EnsembleFactory\n",
    "from polymerist.mdtools.openmmtools.parameters import SimulationParameters\n",
    "from polymerist.mdtools.openmmtools import serialization\n",
    "from polymerist.mdtools.openmmtools.evaluation import eval_openmm_energies_separated\n",
    "\n",
    "from polybuild_utils import initialize_polymer_progress, interchange_to_lammps, interchange_to_openmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "sim_params = SimulationParameters.from_file('sim_params.json')\n",
    "ensfac = EnsembleFactory.from_thermo_params(sim_params.thermo_params)\n",
    "\n",
    "build_lammps : bool = True\n",
    "build_openmm : bool = False\n",
    "lmp_args = [\"-screen\", \"none\"]#, \"-log\", \"none\"] # blocks stdout and log.lammps writes to avoid clutter\n",
    "\n",
    "group, (status_id, curr_compound_id, comp_progress_id) = initialize_polymer_progress(num_compounds=len(mol_file_frame))\n",
    "status_readout, compound_readout, compound_progress = group.renderables\n",
    "\n",
    "energies = RecursiveDict()\n",
    "with Live(group, refresh_per_second=10) as live:\n",
    "    # ensure bars start at 0\n",
    "    for pbar in group.renderables: \n",
    "        for task_id in pbar.task_ids:\n",
    "            pbar.reset(task_id)\n",
    "\n",
    "    # iterate over all distinct chemistries by reaction mechanism\n",
    "    for (mechanism, polymer_name), row in mol_file_frame.iterrows():\n",
    "        comp_desc = f'{polymer_name} ({row.n_atoms_cap} {row.oligomer_type} on {row.lattice_size} lattice)'\n",
    "        # comp_desc = insert_into_text_periodic(comp_desc, period=32)\n",
    "        compound_readout.update(curr_compound_id, polymer_name=comp_desc, mechanism=mechanism)\n",
    "\n",
    "        # load recorded topology and interchange files\n",
    "        if (build_lammps or build_openmm): # if neither is being loaded, don't bother trying to load an Interchange TODO: modify this to check if the directory tree exists\n",
    "            status_readout.update(status_id, action='Loading Interchange from file')\n",
    "            offtop = topology.topology_from_sdf(row.topology_path, allow_undefined_stereo=True)\n",
    "            with row.interchange_path.open('rb') as inc_file:\n",
    "                interchange = pickle.load(inc_file)\n",
    "\n",
    "        # LAMMPS\n",
    "        if build_lammps:\n",
    "            lmp_dir : Path = row.directory / 'LAMMPS'\n",
    "            lmp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            lmp_input_path = assemble_path(lmp_dir, polymer_name, extension='in')\n",
    "            lmp_data_path  = assemble_path(lmp_dir, polymer_name, extension='lammps')\n",
    "            lmp_prof_path  = assemble_path(lmp_dir, polymer_name, extension='txt', postfix='profile')\n",
    "            lmp_md_paths = [lmp_input_path, lmp_data_path]\n",
    "\n",
    "            lmp_input_path.touch()\n",
    "\n",
    "            ## writing LAMMPS files\n",
    "            if not all(path.exists() for path in lmp_md_paths): # avoid double-writing if files are already generated\n",
    "                status_readout.update(status_id, action='Writing LAMMPS files')\n",
    "                with Timer() as lammps_timer:\n",
    "                    lmp_profile = cProfile.Profile()\n",
    "                    lmp_ret = lmp_profile.runcall(\n",
    "                        interchange_to_lammps,\n",
    "                        interchange=interchange,\n",
    "                        lmp_data_path=lmp_data_path,\n",
    "                        lmp_input_path=lmp_input_path\n",
    "                    )\n",
    "                    \n",
    "                lmp_profile.dump_stats(lmp_prof_path)\n",
    "                if row['record_path'].exists():\n",
    "                    append_to_json(row['record_path'], lammps_time=lammps_timer.time_taken)\n",
    "\n",
    "            ## evaluating LAMMPS energies\n",
    "            # box_params = lammpseval.get_lammps_unit_cell(lmp_input_path, cmdargs=lmp_args) # need to make copy of args, list seems to be modified upon pass\n",
    "            status_readout.update(status_id, action='Evaluating LAMMPS structure energies')\n",
    "            energies['LAMMPS'][(mechanism, polymer_name)] = lammpseval.get_lammps_energies(lmp_input_path, preferred_unit=kilojoule_per_mole, cmdargs=lmp_args) # need to make copy of args, list seems to be modified upon pass\n",
    "\n",
    "        # OpenMM\n",
    "        if build_openmm:\n",
    "            omm_dir : Path = row.directory / 'OpenMM'\n",
    "            omm_dir.mkdir(exist_ok=True)\n",
    "\n",
    "            sim_paths = serialization.SimulationPaths()\n",
    "            omm_prof_path  = assemble_path(omm_dir, polymer_name, postfix='profile'   , extension='txt')\n",
    "            omm_integ_path = assemble_path(omm_dir, polymer_name, postfix='integrator', extension='xml')\n",
    "            omm_md_paths = [omm_top_path, omm_sys_path, omm_state_path] = sim_paths.init_top_and_sys_paths(omm_dir, polymer_name)\n",
    "            omm_md_paths += (omm_integ_path,)\n",
    "\n",
    "            if not all(path.exists() for path in omm_md_paths): # avoid double-writing if files are already generated\n",
    "                status_readout.update(status_id, action='Writing OpenMM files')\n",
    "                with Timer() as openmm_timer:\n",
    "                    omm_profile = cProfile.Profile()\n",
    "                    integrator = ensfac.integrator(time_step=sim_params.integ_params.time_step)\n",
    "                    with omm_integ_path.open('w') as integ_file:\n",
    "                        integ_file.write(XmlSerializer.serialize(integrator))\n",
    "\n",
    "                    omm_ret = omm_profile.runcall(\n",
    "                        interchange_to_openmm,\n",
    "                        interchange=interchange,\n",
    "                        integrator=integrator,\n",
    "                        omm_top_path=omm_top_path,\n",
    "                        omm_sys_path=omm_sys_path,\n",
    "                        omm_state_path=omm_state_path\n",
    "                    )\n",
    "\n",
    "                omm_profile.dump_stats(omm_prof_path)\n",
    "                if row['record_path'].exists():\n",
    "                    append_to_json(row['record_path'], openmm_time=openmm_timer.time_taken)\n",
    "                    \n",
    "            ## evaluating OpenMM energies\n",
    "            status_readout.update(status_id, action='Loading OpenMM context')\n",
    "            with omm_sys_path.open('r') as sys_file:\n",
    "                omm_sys_read = XmlSerializer.deserialize(sys_file.read())\n",
    "\n",
    "            with omm_state_path.open('r') as state_file:\n",
    "                omm_state_read = XmlSerializer.deserialize(state_file.read())\n",
    "\n",
    "            with omm_integ_path.open('r') as integ_file:\n",
    "                omm_integ_read = XmlSerializer.deserialize(integ_file.read())\n",
    "\n",
    "            context = Context(omm_sys_read, omm_integ_read)\n",
    "            serialization.apply_state_to_context(context, omm_state_read)\n",
    "\n",
    "            status_readout.update(status_id, action='Evaluating OpenMM energies')\n",
    "            openmm_pot, openmm_kin = eval_openmm_energies_separated(context, preferred_unit=kilojoule_per_mole)\n",
    "            energies['OpenMM'][(mechanism, polymer_name)] = {**openmm_pot, **openmm_kin}\n",
    "\n",
    "        compound_progress.advance(comp_progress_id)\n",
    "        sleep(0.1) # needed to give final bar enough time to catch up\n",
    "        collect() # manual garbage collector call to try to alleviate memory issues\n",
    "    status_readout.update(status_id, action='MD file output complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energy_dir = Path('energy_tables')\n",
    "energy_dir.mkdir(exist_ok=True)\n",
    "edfs = {}\n",
    "\n",
    "for platform, energies_dict in energies.items():\n",
    "    energy_path = assemble_path(energy_dir, MOL_MASTER_DIR.stem, postfix=f'{platform}_energies', extension='csv')\n",
    "    edf = pd.DataFrame.from_dict(energies_dict, orient='index')\n",
    "    edf.to_csv(energy_path)\n",
    "    edfs[platform] = edf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing energies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading energy tables and comparing contributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "from dataclasses import dataclass\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format # disable scientific notation\n",
    "\n",
    "force_name_remap = { # easier-to-understand names for OpenMM energies\n",
    "    'vdW force'                : 'vdW',\n",
    "    'Electrostatics force'     : 'Electrostatic',\n",
    "    'vdW 1-4 force'            : 'vdW 1-4',\n",
    "    'Electrostatics 1-4 force' : 'Electrostatic 1-4',\n",
    "    'PeriodicTorsionForce'     : 'Dihedral',\n",
    "    'HarmonicAngleForce'       : 'Angle',\n",
    "    'HarmonicBondForce'        : 'Bond'\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class TableFormats:\n",
    "    sum_terms : dict[str, list[str]]\n",
    "    del_terms : list[str]\n",
    "\n",
    "formats = {\n",
    "    'OpenMM' : TableFormats(\n",
    "        sum_terms = {\n",
    "            'vdW' : ['vdW', 'vdW 1-4'],\n",
    "            'Coulomb' : ['Electrostatic', 'Electrostatic 1-4']\n",
    "        },\n",
    "        del_terms = ['Kinetic']\n",
    "    ),\n",
    "    'LAMMPS' : TableFormats(\n",
    "        sum_terms = {\n",
    "            'vdW' : ['vdW', 'Dispersion'],\n",
    "            'Dihedral' : ['Proper Torsion', 'Improper Torsion'],\n",
    "            'Coulomb'  : ['Coulomb Short', 'Coulomb Long']\n",
    "        },\n",
    "        del_terms = ['Nonbonded']\n",
    "    ),\n",
    "}\n",
    "\n",
    "# apply reformatting to respective tables\n",
    "edfs_fmt = {}\n",
    "for platform, energy_df in edfs.items():\n",
    "    fmt = formats[platform]\n",
    "\n",
    "    # combine selected terms\n",
    "    new_energy_df = energy_df.copy(deep=True) # leave original unmodified\n",
    "    for combined_contrib, contribs in fmt.sum_terms.items():\n",
    "        new_term = reduce(add, (new_energy_df[contrib] for contrib in contribs)) # merge contributions into a single new named term\n",
    "        new_energy_df = new_energy_df.drop(columns=contribs, inplace=False) # clear contributions\n",
    "        new_energy_df[combined_contrib] = new_term # done after drop to ensure name clashes don't result in extra deletion\n",
    "    \n",
    "    # delete redundant terms\n",
    "    for del_contrib in fmt.del_terms:\n",
    "        new_energy_df.drop(columns=[del_contrib], inplace=True) # clear contributions\n",
    "    edfs_fmt[platform] = new_energy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from polymerist.graphics.plotutils import presize_subplots\n",
    "\n",
    "\n",
    "col_order = ['Bond', 'Angle', 'Dihedral', 'vdW', 'Coulomb', 'Potential']\n",
    "max_err_perc : float = None\n",
    "# max_err_perc : float = 100.0\n",
    "\n",
    "energy_perc_rel_err = ((edfs_fmt['OpenMM'] - edfs_fmt['LAMMPS']) / edfs_fmt['LAMMPS']).abs() * 100\n",
    "if max_err_perc:\n",
    "    err_in_tol = (energy_perc_rel_err.abs() < max_err_perc).all(axis=1)\n",
    "    energy_perc_rel_err = energy_perc_rel_err[err_in_tol]\n",
    "\n",
    "fig, ax = presize_subplots(nrows=2, ncols=3)\n",
    "for col, axis in zip(col_order, ax.flatten()):\n",
    "    heights, bins, patches = axis.hist(energy_perc_rel_err[col], bins=50)\n",
    "    axis.set_ylabel(f'{col} energy (rel. % error)')\n",
    "    axis.tick_params(axis='x', rotation=-20)\n",
    "    \n",
    "plt.show()\n",
    "display(energy_perc_rel_err[col_order])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_path = assemble_path(energy_dir, 'Energy_rel_err_table', postfix=f'{MOL_MASTER_DIR.stem}_{lattice_size}', extension='csv')\n",
    "energy_perc_rel_err.to_csv(diff_path)\n",
    "\n",
    "energy_fig_path = assemble_path(energy_dir, 'Energy_rel_err_graphs', postfix=f'{MOL_MASTER_DIR.stem}_{lattice_size}', extension='png')\n",
    "fig.savefig(energy_fig_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting just the systems which have density data available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polymer_name(row : pd.Series) -> str:\n",
    "    return f'poly({row[\"IUPAC_name_monomer_0\"]}-co-{row[\"IUPAC_name_monomer_1\"]})'.lower()\n",
    "\n",
    "p = Path('monomer_data_processed/monomer_data_MASTER.csv')\n",
    "polyid_df = pd.read_csv(p, index_col=0)\n",
    "\n",
    "polyid_df['polymer_name'] = polyid_df.apply(get_polymer_name, axis=1) # generate column of polymer names from monomer names\n",
    "polyid_df.set_index(['rxn_name', 'polymer_name'], inplace=True) # reindex by mechanism and molecule name\n",
    "polyid_df = polyid_df[polyid_df['Density'].notnull()] # filter by density values\n",
    "\n",
    "common_index = polyid_df.index.intersection(energy_perc_rel_err.index)\n",
    "polyid_df.loc[common_index]['Density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_density = energy_perc_rel_err.loc[common_index]\n",
    "has_density['Density'] = polyid_df.loc[common_index]['Density']\n",
    "has_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openff-dev-updated",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
